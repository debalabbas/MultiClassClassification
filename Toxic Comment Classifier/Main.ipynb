{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toxic Comment Classifier\n",
    "\n",
    "\n",
    "Welcome to this jupyter notebook, here we will try to make a model to classify given pieces of text for multiple labels using a single input multiple output model, to classify whether given piece of comment is one or more than one of the following labels\n",
    "\n",
    "- toxic\n",
    "- severe_toxic\n",
    "- obscene\n",
    "- threat\n",
    "- insult\n",
    "- identity_hate\n",
    "\n",
    "This problem ([Toxic Comment Classifier](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/overview)) is hosted on kaggle\n",
    "\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [Part 1 Data Preparation](#1)\n",
    "     - [1.1 Importing the Data](#1.1)\n",
    "     - [1.2 Tokenizing and Padding the Data](#1.2)\n",
    "     \n",
    "- [Part 2. Creating and Training the Model ](#2)\n",
    "    - [2.1 Creating The Model](#2.1)\n",
    "    - [2.2 Compiling and Training the Model](#2.2)\n",
    "    \n",
    "- [Part 3. Testing The Model](#3)\n",
    "    - [3.1 Loading the Test Data](#3.1)\n",
    "    - [3.2 Tokenizing and Padding the Data](#3.2)\n",
    "    - [3.3 Predicting on the Test Data](#3.3)\n",
    "    - [3.4 Final Result](#3.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1'></a>\n",
    "## 1 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "### 1.1 Importing the Data\n",
    "\n",
    "We will first import the training dataset which has 8 columns `id,comment_text,toxic,sever_toxic,obscene,threat,insult,identity_hate` where the last 6 are the target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('train.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       "       \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       "       \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       "       '\"\\nMore\\nI can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"',\n",
       "       \"You, sir, are my hero. Any chance you remember what page that's on?\",\n",
       "       '\"\\n\\nCongratulations from me as well, use the tools well. \\xa0· talk \"',\n",
       "       'COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK',\n",
       "       \"Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.\",\n",
       "       \"Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\",\n",
       "       'alignment on this subject and which are contrary to those of DuLithgow'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = df.comment_text.values\n",
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "\n",
    "### 1.2 Tokenizing and Padding the Data\n",
    "\n",
    "First we will start by removing the `\\n`and `\\t` tags from the text thrn we will tokenize the data using the tensorflow [Tokenizer](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/preprocessing/text/Tokenizer) and then padding the input so that all the training samples become uniform using the [pad_sequence](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) function from tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(text):\n",
    "    \n",
    "    text = re.sub('\\n',' ',text)\n",
    "    text = re.sub('\\t',' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [remove(x) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Explanation Why the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\",\n",
       " \"D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)\",\n",
       " \"Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.\",\n",
       " '\" More I can\\'t make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.  There appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It\\'s listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"',\n",
       " \"You, sir, are my hero. Any chance you remember what page that's on?\",\n",
       " '\"  Congratulations from me as well, use the tools well. \\xa0· talk \"',\n",
       " 'COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK',\n",
       " \"Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.\",\n",
       " \"Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169\",\n",
       " 'alignment on this subject and which are contrary to those of DuLithgow']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will tokenize the data the `oov_token` it is used to replace out of vocabulary word when the tokenizer is called to tokenize a given piece of text where vocabulary is the mapping of words to index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='<UNK>') # Tokenizing\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = tokenizer.word_index\n",
    "texts = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are trying to find the right length to be used as the uniform lengths for all the training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    159571.000000\n",
       "mean         68.221569\n",
       "std         101.073763\n",
       "min           1.000000\n",
       "25%          17.000000\n",
       "50%          36.000000\n",
       "75%          76.000000\n",
       "max        1403.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = pd.Series([len(x) for x in texts])\n",
    "lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic     = df.toxic.values\n",
    "sev_toxic = df.severe_toxic.values\n",
    "obs       = df.obscene.values\n",
    "threat    = df.threat.values\n",
    "ins       = df.insult.values\n",
    "idn_hate  = df.identity_hate.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensors = pad_sequences(texts,maxlen=128,padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2'></a>\n",
    "## 2. Creating and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "### 2.1 Creating the Model\n",
    "\n",
    "The following layers are used in the creation of the model\n",
    "\n",
    "\n",
    "- [tf.keras.layers.Input()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/InputLayer): It is used to instantiate a Keras tensor which is used as an input to the model,in our case the shape ofthe tensor will be (batch_size,128)`tf.keras.layers.Input(shape=(128,))`\n",
    "\n",
    "\n",
    "- [tf.keras.layers.Embedding()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/Embedding): this converts each token to its vector representation. In this case, it is the the size of the vocabulary by the dimension of the model: `tf.keras.layers.Embedding(vocab_size, d_model)`. `vocab_size` is the number of entries in the given vocabulary. `d_model` is the number of elements in the word embedding. \n",
    "\n",
    "- [tf.keras.layers.LSTM()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/LSTM): LSTM layer of size d_model and which returns a sequence\n",
    "\n",
    "- [tf.tf.keras.layers.GlobalAveragePooling1D()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) : Global average pooling operation for temporal data.\n",
    "\n",
    "- [tf.keras.layers.BatchNormalization()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/BatchNormalization):Normalize and scale inputs or activations.\n",
    "\n",
    "- [tf.keras.layers.Dropout()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/Dropout):Applies Dropout to the input.(Prevents from overfitting by deactivating a fraction of neurons from the previous layer)\n",
    "\n",
    "- [tf.keras.layers.Dense()](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/layers/Dense): We have used a Dense layer with [Relu](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/activations/relu) activation and 6 output Dense layer with [Sigmoid](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/activations/sigmoid) activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(num_words = len(vocab) + 1, d_model = 128,droput=0.1):\n",
    "    \n",
    "    inp = tf.keras.layers.Input(shape=(128,))\n",
    "    \n",
    "    x   = tf.keras.layers.Embedding(num_words,d_model)(inp)\n",
    "    x   = tf.keras.layers.LSTM(d_model,return_sequences=True)(x)\n",
    "    x   = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x   = tf.keras.layers.BatchNormalization()(x)\n",
    "    x   = tf.keras.layers.Dropout(droput)(x)\n",
    "    x   = tf.keras.layers.Dense(64,'relu')(x)\n",
    "    \n",
    "    tox     = tf.keras.layers.Dense(1,'sigmoid',name='Toxic_Classifier')(x)\n",
    "    sev_tox = tf.keras.layers.Dense(1,'sigmoid',name='Severe_Toxic_Classifier')(x)\n",
    "    obs     = tf.keras.layers.Dense(1,'sigmoid',name='Obscene_Classifier')(x)\n",
    "    thr     = tf.keras.layers.Dense(1,'sigmoid',name='Threat_Classifier')(x)\n",
    "    ins     = tf.keras.layers.Dense(1,'sigmoid',name='Insult_Classifier')(x)\n",
    "    idh     = tf.keras.layers.Dense(1,'sigmoid',name='Identity_Hate_Classifier')(x)\n",
    "    \n",
    "    Model = tf.keras.models.Model(inp,[tox,sev_tox,obs,thr,ins,idh])\n",
    "    \n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 128, 128)     26923392    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 128, 128)     131584      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 128)          0           lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 128)          512         global_average_pooling1d_3[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 128)          0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 64)           8256        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Toxic_Classifier (Dense)        (None, 1)            65          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Severe_Toxic_Classifier (Dense) (None, 1)            65          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Obscene_Classifier (Dense)      (None, 1)            65          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Threat_Classifier (Dense)       (None, 1)            65          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Insult_Classifier (Dense)       (None, 1)            65          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Identity_Hate_Classifier (Dense (None, 1)            65          dense_8[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 27,064,134\n",
      "Trainable params: 27,063,878\n",
      "Non-trainable params: 256\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Model = Classifier()\n",
    "Model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "### 2.2 Compiling and Training the Model\n",
    "We are going to use the [Adam](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/optimizers/Adam) optimizer and [Binary Crossentropy](https://www.tensorflow.org/versions/r2.2/api_docs/python/tf/keras/losses/BinaryCrossentropy) loss for all the predictions and train the model for 5 epochs with 0.1 Validation split and with 100 steps per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model.compile(optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "              loss={'Toxic_Classifier':tf.keras.losses.BinaryCrossentropy(),\n",
    "                    'Severe_Toxic_Classifier':tf.keras.losses.BinaryCrossentropy(),\n",
    "                    'Obscene_Classifier':tf.keras.losses.BinaryCrossentropy(),\n",
    "                    'Threat_Classifier':tf.keras.losses.BinaryCrossentropy(),\n",
    "                    'Insult_Classifier':tf.keras.losses.BinaryCrossentropy(),\n",
    "                    'Identity_Hate_Classifier':tf.keras.losses.BinaryCrossentropy()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100/100 [==============================] - 44s 441ms/step - loss: 0.6588 - Toxic_Classifier_loss: 0.2062 - Severe_Toxic_Classifier_loss: 0.0613 - Obscene_Classifier_loss: 0.1459 - Threat_Classifier_loss: 0.0491 - Insult_Classifier_loss: 0.1371 - Identity_Hate_Classifier_loss: 0.0593 - val_loss: 0.5494 - val_Toxic_Classifier_loss: 0.2108 - val_Severe_Toxic_Classifier_loss: 0.0275 - val_Obscene_Classifier_loss: 0.1282 - val_Threat_Classifier_loss: 0.0189 - val_Insult_Classifier_loss: 0.1298 - val_Identity_Hate_Classifier_loss: 0.0341\n",
      "Epoch 2/5\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.2636 - Toxic_Classifier_loss: 0.0826 - Severe_Toxic_Classifier_loss: 0.0241 - Obscene_Classifier_loss: 0.0498 - Threat_Classifier_loss: 0.0142 - Insult_Classifier_loss: 0.0622 - Identity_Hate_Classifier_loss: 0.0308 - val_loss: 0.3882 - val_Toxic_Classifier_loss: 0.1484 - val_Severe_Toxic_Classifier_loss: 0.0209 - val_Obscene_Classifier_loss: 0.0750 - val_Threat_Classifier_loss: 0.0194 - val_Insult_Classifier_loss: 0.0907 - val_Identity_Hate_Classifier_loss: 0.0337\n",
      "Epoch 3/5\n",
      "100/100 [==============================] - 41s 411ms/step - loss: 0.2093 - Toxic_Classifier_loss: 0.0550 - Severe_Toxic_Classifier_loss: 0.0214 - Obscene_Classifier_loss: 0.0380 - Threat_Classifier_loss: 0.0130 - Insult_Classifier_loss: 0.0537 - Identity_Hate_Classifier_loss: 0.0282 - val_loss: 0.4029 - val_Toxic_Classifier_loss: 0.1557 - val_Severe_Toxic_Classifier_loss: 0.0206 - val_Obscene_Classifier_loss: 0.0801 - val_Threat_Classifier_loss: 0.0172 - val_Insult_Classifier_loss: 0.0961 - val_Identity_Hate_Classifier_loss: 0.0331\n",
      "Epoch 4/5\n",
      "100/100 [==============================] - 41s 412ms/step - loss: 0.1732 - Toxic_Classifier_loss: 0.0422 - Severe_Toxic_Classifier_loss: 0.0204 - Obscene_Classifier_loss: 0.0298 - Threat_Classifier_loss: 0.0107 - Insult_Classifier_loss: 0.0465 - Identity_Hate_Classifier_loss: 0.0236 - val_loss: 0.3127 - val_Toxic_Classifier_loss: 0.1114 - val_Severe_Toxic_Classifier_loss: 0.0204 - val_Obscene_Classifier_loss: 0.0623 - val_Threat_Classifier_loss: 0.0136 - val_Insult_Classifier_loss: 0.0784 - val_Identity_Hate_Classifier_loss: 0.0266\n",
      "Epoch 5/5\n",
      "100/100 [==============================] - 41s 410ms/step - loss: 0.1393 - Toxic_Classifier_loss: 0.0324 - Severe_Toxic_Classifier_loss: 0.0189 - Obscene_Classifier_loss: 0.0222 - Threat_Classifier_loss: 0.0090 - Insult_Classifier_loss: 0.0392 - Identity_Hate_Classifier_loss: 0.0175 - val_loss: 0.3350 - val_Toxic_Classifier_loss: 0.1293 - val_Severe_Toxic_Classifier_loss: 0.0220 - val_Obscene_Classifier_loss: 0.0656 - val_Threat_Classifier_loss: 0.0131 - val_Insult_Classifier_loss: 0.0780 - val_Identity_Hate_Classifier_loss: 0.0270\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1abaafbd310>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Model.fit(tensors,[toxic,sev_toxic,obs,threat,ins,idn_hate],epochs=5,validation_split=0.1,steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3'></a>\n",
    "## 3. Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "\n",
    "### 3.1 Importing The Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text\n",
       "0  00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...\n",
       "1  0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...\n",
       "2  00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...\n",
       "3  00017563c3f7919a  :If you have a look back at the source, the in...\n",
       "4  00017695ad8997eb          I don't anonymously edit articles at all."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('test.csv') # Reading the test data set\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_comment = df_test.comment_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.,\",\n",
       "       '== From RfC == \\n\\n The title is fine as it is, IMO.',\n",
       "       '\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lapland —  /  \"',\n",
       "       \":If you have a look back at the source, the information I updated was the correct form. I can only guess the source hadn't updated. I shall update the information once again but thank you for your message.\",\n",
       "       \"I don't anonymously edit articles at all.\"], dtype=object)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_comment[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "### 3.2 Tokenizing and Padding the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensors = tokenizer.texts_to_sequences(test_comment) # tokenizing the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tensors = pad_sequences(test_tensors,maxlen=128,padding='post',truncating='post') #padding the tensors to a length of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 128)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.3'></a>\n",
    "\n",
    "### 3.3 Predicting on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = Model.predict(test_tensors)  # predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tox = y_hat[0]\n",
    "test_sev_tox = y_hat[1]\n",
    "test_obs = y_hat[2]\n",
    "test_threat = y_hat[3]\n",
    "test_ins = y_hat[4]\n",
    "test_idn_hate = y_hat[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({'id':df_test.id.values,\n",
    "                       'toxic':np.squeeze(test_tox),\n",
    "                       'severe_toxic':np.squeeze(test_sev_tox),\n",
    "                       'obscene':np.squeeze(test_obs),\n",
    "                       'threat':np.squeeze(test_threat),\n",
    "                       'insult':np.squeeze(test_ins),\n",
    "                       'identity_hate':np.squeeze(test_idn_hate)})   # Creating the submission dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>0.996190</td>\n",
       "      <td>0.541441</td>\n",
       "      <td>0.986939</td>\n",
       "      <td>0.082921</td>\n",
       "      <td>0.941591</td>\n",
       "      <td>0.315915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>0.000320</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     toxic  severe_toxic   obscene    threat    insult  \\\n",
       "0  00001cee341fdb12  0.996190      0.541441  0.986939  0.082921  0.941591   \n",
       "1  0000247867823ef7  0.002316      0.000010  0.000061  0.000237  0.000214   \n",
       "2  00013b17ad220c46  0.000320      0.000007  0.000016  0.000109  0.000039   \n",
       "3  00017563c3f7919a  0.000068      0.000003  0.000022  0.000016  0.000017   \n",
       "4  00017695ad8997eb  0.000363      0.000005  0.000024  0.000120  0.000054   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.315915  \n",
       "1       0.000043  \n",
       "2       0.000021  \n",
       "3       0.000003  \n",
       "4       0.000015  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='3.4'></a>\n",
    "## 3.4 Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='accuracy.png'>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
